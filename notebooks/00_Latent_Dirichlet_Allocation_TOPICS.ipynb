{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd1e44c",
   "metadata": {},
   "source": [
    "<div style=\"font-size:30pt\">LDA for Sentiment Analysis</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defa9f3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7995a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/victor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bef964",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bfe1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona_NLP_test.csv  Corona_NLP_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11277c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../raw_data/Corona_NLP_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea1ebe",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41dba7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerize(df, label):\n",
    "    \"\"\" text lowercase\n",
    "        removes \\n\n",
    "        removes \\t\n",
    "        removes \\r \"\"\"\n",
    "    df[label] = df[label].str.lower()\n",
    "    df[label] = df[label].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "    df[label] = df[label].apply(lambda x: x.replace(\"\\r\", \" \"))\n",
    "    df[label] = df[label].apply(lambda x: x.replace(\"\\t\", \" \"))\n",
    "    return df\n",
    "\n",
    "def remove_emails(df, label):\n",
    "    \"\"\" This function removes email adresses\n",
    "        inputs:\n",
    "         - text \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\", \" \", x))\n",
    "    return df\n",
    "\n",
    "def remove_mentions(df, label):\n",
    "    \"\"\" This function removes mentions (Twitter - starting with @) from texts\n",
    "        inputs:\n",
    "         - text \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", x))\n",
    "    return df\n",
    "\n",
    "def remove_hyperlinks(df, label):\n",
    "    \"\"\" This function removes hyperlinks from texts\n",
    "        inputs:\n",
    "         - text \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"http\\S+\", \" \", x))\n",
    "    return df\n",
    "\n",
    "def remove_hashtags(df, label):\n",
    "    \"\"\" This function removes hashtags\n",
    "        inputs:\n",
    "         - text \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"#\\w+\", \" \", x))\n",
    "    return df\n",
    "\n",
    "def remove_html_tags(df, label):\n",
    "    \"\"\" This function removes html tags from texts\n",
    "        inputs:\n",
    "         - text \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"<.*?>\", \" \", x))\n",
    "    return df\n",
    "\n",
    "def remove_numbers(df, label):\n",
    "    \"\"\" This function removes numbers from a text\n",
    "        inputs:\n",
    "         - text \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
    "    return df\n",
    "\n",
    "def encode_unknown(df, label):\n",
    "    \"\"\" This function encodes special caracters \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: unicodedata.normalize(\"NFD\", x).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    return df\n",
    "\n",
    "def clean_punctuation_no_accent(df, label):\n",
    "    \"\"\" This function removes punctuation and accented characters from texts in a dataframe \n",
    "        To be appplied to languages that have no accents, ex: english \n",
    "    \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "    return df\n",
    "\n",
    "def remove_stop_words(text, stopwords=set(stopwords.words('english'))):\n",
    "    \"\"\" This function removes stop words from a text\n",
    "        inputs:\n",
    "         - stopword list\n",
    "         - text \"\"\"\n",
    "\n",
    "    # prepare new text\n",
    "    text_splitted = text.split(\" \")\n",
    "    text_new = list()\n",
    "    \n",
    "    # stop words updated\n",
    "    #stopwords = stopwords.union({\"grocery store\", \"covid\", \"supermarket\", \"people\", \"grocery\", \"store\", \"price\", \"time\"})\n",
    "    \n",
    "    # loop\n",
    "    for word in text_splitted:\n",
    "        if word not in stopwords:\n",
    "            text_new.append(word)\n",
    "    return \" \".join(text_new)\n",
    "\n",
    "def clean_stopwords(df, label):\n",
    "    \"\"\" This function removes stopwords \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: remove_stop_words(x))\n",
    "    return df\n",
    "\n",
    "def more_cleaning(df, label):\n",
    "    \"\"\" This function\n",
    "     1) removes remaining one-letter words and two letters words\n",
    "     2) replaces multiple spaces by one single space\n",
    "     3) drop empty lines \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', \" \", x))\n",
    "    df[label] = df[label].apply(lambda x: re.sub(r\"[ \\t]{2,}\", \" \", x))\n",
    "    df[label] = df[label].apply(lambda x: x if len(x) != 1 else '')\n",
    "    df[label] = df[label].apply(lambda x: np.nan if x == '' else x)\n",
    "    df = df.dropna(subset=[label], axis=0).reset_index(drop=True).copy()\n",
    "    return df\n",
    "\n",
    "def lemmatize_one_text(text):\n",
    "    \"\"\" This function lemmatizes words in text (it changes word to most close root word)\n",
    "        inputs:\n",
    "         - lemmatizer\n",
    "         - text \"\"\"\n",
    "\n",
    "    # initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # tags\n",
    "    lem_tags = ['a', 'r', 'n', 'v']\n",
    "\n",
    "    # prepare new text\n",
    "    text_splitted = text.split(\" \")\n",
    "    text_new = list()\n",
    "\n",
    "    # change bool\n",
    "    changed = ''\n",
    "    \n",
    "    # loop\n",
    "    for word in text_splitted:\n",
    "        changed = ''\n",
    "        for tag in lem_tags:\n",
    "            if lemmatizer.lemmatize(word, tag) != word:\n",
    "                changed = tag\n",
    "        if changed == '':\n",
    "            text_new.append(word)\n",
    "        else:\n",
    "            text_new.append(lemmatizer.lemmatize(word, changed))\n",
    "\n",
    "    return \" \".join(text_new)\n",
    "\n",
    "def lemmatize(df, label):\n",
    "    \"\"\" This function lemmatizes texts \"\"\"\n",
    "    df[label] = df[label].apply(lambda x: lemmatize_one_text(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18946e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"CleanTweet\"] = df_train[\"OriginalTweet\"]\n",
    "df_train = lowerize(df_train, \"CleanTweet\")\n",
    "df_train = remove_emails(df_train, \"CleanTweet\")\n",
    "df_train = remove_mentions(df_train, \"CleanTweet\")\n",
    "df_train = remove_hyperlinks(df_train, \"CleanTweet\")\n",
    "df_train = remove_hashtags(df_train, \"CleanTweet\")\n",
    "df_train = remove_html_tags(df_train, \"CleanTweet\")\n",
    "df_train = remove_numbers(df_train, \"CleanTweet\")\n",
    "df_train = encode_unknown(df_train, \"CleanTweet\")\n",
    "df_train = clean_punctuation_no_accent(df_train, \"CleanTweet\")\n",
    "df_train = clean_stopwords(df_train, \"CleanTweet\")\n",
    "df_train = more_cleaning(df_train, \"CleanTweet\")\n",
    "df_train = lemmatize(df_train, \"CleanTweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd3d3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CleanTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17469</th>\n",
       "      <td>21296</td>\n",
       "      <td>66248</td>\n",
       "      <td>Lansing, MI</td>\n",
       "      <td>23-03-2020</td>\n",
       "      <td>Order on food: \\r\\r\\n\\r\\r\\n\"As needed, however...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>order food needed however individual may leave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14205</th>\n",
       "      <td>18025</td>\n",
       "      <td>62977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21-03-2020</td>\n",
       "      <td>Can you imagine doing this in todays climate?...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>imagine today climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39009</th>\n",
       "      <td>42864</td>\n",
       "      <td>87816</td>\n",
       "      <td>EMEIA</td>\n",
       "      <td>12-04-2020</td>\n",
       "      <td>We're in the 'hair color' phase of panic buyi...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hair color phase panic buying first went hand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName     Location     TweetAt  \\\n",
       "17469     21296       66248  Lansing, MI  23-03-2020   \n",
       "14205     18025       62977          NaN  21-03-2020   \n",
       "39009     42864       87816        EMEIA  12-04-2020   \n",
       "\n",
       "                                           OriginalTweet Sentiment  \\\n",
       "17469  Order on food: \\r\\r\\n\\r\\r\\n\"As needed, however...  Positive   \n",
       "14205  Can you imagine doing this in todays climate?...   Neutral   \n",
       "39009  We're in the 'hair color' phase of panic buyi...   Neutral   \n",
       "\n",
       "                                              CleanTweet  \n",
       "17469  order food needed however individual may leave...  \n",
       "14205                             imagine today climate   \n",
       "39009   hair color phase panic buying first went hand...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0066db",
   "metadata": {},
   "source": [
    "# Sentiment column preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a064d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_sen(sentiment):\n",
    "    if sentiment == \"Extremely Positive\":\n",
    "        return 'positive'\n",
    "    elif sentiment == \"Extremely Negative\":\n",
    "        return 'negative'\n",
    "    elif sentiment == \"Positive\":\n",
    "        return 'positive'\n",
    "    elif sentiment == \"Negative\":\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069c5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Sentiment\"] = df_train[\"Sentiment\"].apply(change_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed3e91",
   "metadata": {},
   "source": [
    "# Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae7d3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_positive = CountVectorizer().fit(df_train[df_train[\"Sentiment\"] == \"positive\"][\"CleanTweet\"])\n",
    "vectorizer_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e79e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_neutral = CountVectorizer().fit(df_train[df_train[\"Sentiment\"] == \"neutral\"][\"CleanTweet\"])\n",
    "vectorizer_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645addf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_negative = CountVectorizer().fit(df_train[df_train[\"Sentiment\"] == \"negative\"][\"CleanTweet\"])\n",
    "vectorizer_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f308f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18858\n",
      "10530\n",
      "17581\n"
     ]
    }
   ],
   "source": [
    "data_vectorized_positive = vectorizer_positive.transform(df_train[df_train[\"Sentiment\"] == \"positive\"][\"CleanTweet\"])\n",
    "print(len(data_vectorized_positive.toarray()[0]))\n",
    "data_vectorized_neutral = vectorizer_neutral.transform(df_train[df_train[\"Sentiment\"] == \"neutral\"][\"CleanTweet\"])\n",
    "print(len(data_vectorized_neutral.toarray()[0]))\n",
    "data_vectorized_negative = vectorizer_negative.transform(df_train[df_train[\"Sentiment\"] == \"negative\"][\"CleanTweet\"])\n",
    "print(len(data_vectorized_negative.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f08fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_positive = LatentDirichletAllocation(n_components=2).fit(data_vectorized_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2112b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_neutral = LatentDirichletAllocation(n_components=2).fit(data_vectorized_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3cb91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_negative = LatentDirichletAllocation(n_components=2).fit(data_vectorized_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44675f53",
   "metadata": {},
   "source": [
    "# Get Topic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1694b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-10 - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969f76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('store', 3943.802621528201), ('grocery', 3158.314807422102), ('supermarket', 3085.8866654171984), ('people', 2099.529586992987), ('worker', 1665.7969837901258), ('like', 1439.171590656096), ('covid', 1304.8686831002797), ('amp', 1300.1123559260461), ('shopping', 1256.5253306073187), ('get', 1161.1337772308116)]\n",
      "Topic 1:\n",
      "[('covid', 3356.1313168996394), ('price', 3117.554737199577), ('consumer', 2311.428356038424), ('food', 1728.1020196157497), ('amp', 1356.8876440738834), ('help', 1111.4249687769986), ('hand', 1105.3576395263813), ('pandemic', 984.9920017503847), ('sanitizer', 973.2630721734497), ('demand', 972.4222005026666)]\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model_positive, vectorizer_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2463330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('covid', 1490.4487364796548), ('price', 1305.5495105754226), ('consumer', 1048.2785332285998), ('online', 664.2248034675271), ('shopping', 629.985066016503), ('pandemic', 415.60525174371116), ('food', 353.88362973066603), ('coronavirus', 341.1404325738438), ('amp', 331.1292988706779), ('toilet', 322.7646644921277)]\n",
      "Topic 1:\n",
      "[('store', 1487.9476269625882), ('supermarket', 1329.7501345168455), ('grocery', 1188.1814376525217), ('people', 467.243087204377), ('get', 395.0277400937095), ('covid', 300.55126352032414), ('food', 291.11637026931214), ('need', 254.12215823356323), ('time', 249.5531315003127), ('worker', 244.3341702714723)]\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model_neutral, vectorizer_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af3a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('price', 4303.336242017323), ('covid', 2397.4091800709566), ('consumer', 1658.1439892806056), ('demand', 1184.7285845538202), ('crisis', 1054.3879496766315), ('oil', 995.4921654198821), ('amp', 892.5351563760489), ('pandemic', 830.240891890073), ('due', 626.9841450919477), ('market', 584.1787023808745)]\n",
      "Topic 1:\n",
      "[('food', 3329.498560123918), ('supermarket', 2873.7807816967047), ('store', 2669.4946683459966), ('people', 2603.6929040523346), ('grocery', 2176.4932411969867), ('panic', 2064.2479776662212), ('covid', 1814.590819928993), ('buying', 1255.231402359625), ('get', 1171.1369366868016), ('need', 1157.8618067195448)]\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model_negative, vectorizer_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2392229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
